{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install snscrape\n",
    "# !pip install textblob\n",
    "# !pip install pandas\n",
    "# !pip install vaderSentiment\n",
    "# !pip install nltk\n",
    "# !pip install transformers\n",
    "# !pip install torch\n",
    "# !pip install openai"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Needed imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from tqdm import tnrange, tqdm_notebook, tqdm\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import regex as re\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Start Mining Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"(crash, OR crashing, OR cair, OR queda, OR subir, OR subida, OR bullish, OR bearish, OR explode, OR exploding) -#BTC -#SafeBlast -#bitcoin -#SOL -#solana -#ADA -#XRP -#SHIB -#BNB -giveaway -congrats -congratulations -giving -link (#eth) until:2022-09-15 since:2022-08-15\"\n",
    "# # query = \"(crash, OR crashing, OR cair, OR queda, OR subir, OR subida, OR bullish, OR bearish, OR explode, OR exploding) -#BTC -#SafeBlast -#bitcoin -#SOL -#solana -#ADA -#XRP -#SHIB -#BNB -giveaway -giveaways -congrats -congratulations -winner -giving -link -https -telegram (#eth) until:2022-09-15 since:2022-08-15\"\n",
    "# tweets = []\n",
    "# limit = 1000\n",
    "\n",
    "# for tweet in sntwitter.TwitterHashtagScraper(query).get_items():\n",
    "    \n",
    "#     if len(tweets) == limit:\n",
    "#         break\n",
    "#     else:\n",
    "#         tweets.append([tweet.date, tweet.url, tweet.user.username, tweet.sourceLabel, tweet.user.location, tweet.content, tweet.likeCount, tweet.retweetCount,  tweet.quoteCount, tweet.replyCount])\n",
    "        \n",
    "# df = pd.DataFrame(tweets, columns=['Date', 'TweetURL','User', 'Source', 'Location', 'Tweet', 'Likes_Count','Retweet_Count', 'Quote_Count', 'Reply_Count'])\n",
    "\n",
    "# df.to_csv('../data/bullishTweets.csv')\n",
    "\n",
    "# print(\"Shape: \", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/bullishTweets.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Sentiment Analysis with VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 5512.13it/s]\n"
     ]
    }
   ],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "compound = []\n",
    "for i,s in enumerate(tqdm(df['Tweet'])):\n",
    "    vs = analyzer.polarity_scores(s)\n",
    "    compound.append(vs[\"compound\"])\n",
    "df[\"compoundVader\"] = compound\n",
    "df.head(2)\n",
    "\n",
    "df.to_csv('../data/compoundAnalysis.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis with TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 2002.70it/s]\n"
     ]
    }
   ],
   "source": [
    "compound = []\n",
    "for i,s in enumerate(tqdm(df['Tweet'])):\n",
    "    vs = TextBlob(s).sentiment\n",
    "    compound.append(vs)\n",
    "df[\"compoundTextBlob\"] = compound\n",
    "df.head(2)\n",
    "\n",
    "df.to_csv('../data/compoundAnalysis.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sort vader compound values by descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.sort_values(by=['compoundVader'], ascending=False)\n",
    "df2.to_csv('../data/orderedAnalysis.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate mean compound value (pensar numa maneira melhor de ver isto, mas para já faz o serviço)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  0.15064550000000168\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for x in df2['compoundVader']:\n",
    "    i += x\n",
    "\n",
    "mean = i/len(df2['compoundVader'])\n",
    "print(\"Mean: \", mean)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Text Cleaning using regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleantxt(text):\n",
    "    text= re.sub(r'@[A-Za-z0-9]+', '',text)# removed @mentions\n",
    "    text= re.sub(r'#', '',text)# removed # symbol\n",
    "    text = re.sub(r'RT[\\s]+', '',text)# rmoved RT\n",
    "    text = re.sub(r'https?:\\/\\/\\s+', '',text)# removed the hyperlink\n",
    "    text = re.sub(r'\\w+:\\/\\/[a-zA-Z0-9.\\/-]+', '',text) # removed any other links (like telegram)\n",
    "    text = text.replace('\\r', '').replace('\\n', ' ').replace('\\n', ' ').lower() #remove \\n and \\r an\n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r'', text) #remove non utf8/ascii characters such as '\\x9a\\x91\\x97\\\n",
    "    return text\n",
    "\n",
    "\n",
    "df2[\"Tweet\"] = df2[\"Tweet\"].apply(cleantxt)\n",
    "\n",
    "df2.to_csv('../data/cleanedTweets.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Sentiment Analysis using roBERTa Pretrained Model\n",
    "\n",
    "As the score is returned as an array: [`negative_Value`, `neutral_Value`, `positive_value`]\n",
    "\n",
    "The compound is calculated by `positive_Value` - `negative_Value`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [02:04<00:00,  8.05it/s]\n"
     ]
    }
   ],
   "source": [
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "\n",
    "\n",
    "def sentiment_score(text):\n",
    "    text = str(text)\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    return scores\n",
    "\n",
    "# classifier = pipeline('sentiment-analysis')\n",
    "# def sentiment_score2(text):\n",
    "#     res = classifier(text)\n",
    "#     if res[0]['label'] == 'POSITIVE':\n",
    "#         return res[0]['score']\n",
    "#     elif res[0]['label'] == 'NEGATIVE':\n",
    "#         return -res[0]['score']\n",
    "#     else:\n",
    "#         return 0\n",
    "\n",
    "\n",
    "scores = []\n",
    "for i,s in enumerate(tqdm(df2['Tweet'])):\n",
    "    res = sentiment_score(s)\n",
    "    scores.append(res[2] - res[0])\n",
    "\n",
    "df2[\"compoundRoBERTa\"] = scores\n",
    "df2.head(2)\n",
    "\n",
    "df2.to_csv('../data/roBERTaAnalysis.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### NLTK pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import string\n",
    "\n",
    "# Definir as stopwords da língua portuguesa\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Definir o stemmer a ser utilizado\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "# Definir a função de pré-processamento\n",
    "def preprocessamento(tweet):\n",
    "    # Tokenização\n",
    "    tokens = word_tokenize(tweet.lower())\n",
    "    # Remoção de stopwords e caracteres especiais\n",
    "    tokens = [token for token in tokens if (token not in stopwords) and (token not in string.punctuation)]\n",
    "    # Stemming\n",
    "    tokens_stem = [stemmer.stem(token) for token in tokens]\n",
    "    # Juntar tokens em uma string\n",
    "    tweet_preprocessado = \" \".join(tokens_stem)\n",
    "    return tweet_preprocessado\n",
    "\n",
    "df2['Tweet_NLTK'] = df2['Tweet'].apply(preprocessamento)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Classifying as Positive or Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma nova coluna \"sentiment\" com valores \"positive\" ou \"negative\"\n",
    "df2[\"Sentiment\"] = [\"positive\" if x >= 0 else \"negative\" for x in df2[\"compoundVader\"]]\n",
    "\n",
    "\n",
    "df2.to_csv('../data/preprocessedTweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([datetime.date(2022, 9, 9), datetime.date(2022, 9, 11),\n",
       "       datetime.date(2022, 9, 13), datetime.date(2022, 9, 12),\n",
       "       datetime.date(2022, 9, 14), datetime.date(2022, 9, 10),\n",
       "       datetime.date(2022, 9, 7), datetime.date(2022, 9, 8),\n",
       "       datetime.date(2022, 9, 6)], dtype=object)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Definir a função de extração da data\n",
    "def extrair_data(data_str):\n",
    "    data = datetime.strptime(data_str, \"%Y-%m-%d %H:%M:%S%z\").date()\n",
    "    return data\n",
    "\n",
    "# Aplicar a função à coluna \"Date\" do DataFrame\n",
    "df2[\"Date\"].apply(extrair_data).unique()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted classification for each Tweet considering num of interactions (likes and retweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = []\n",
    "# for i, s in tqdm(df2.iterrows(), total=df2.shape[0]):\n",
    "#     scores.append(s[\"compoundVader\"] * ((s[\"Likes_Count\"]+1))* ((s[\"Retweet_Count\"]+1)))\n",
    "# df2[\"score\"] = scores\n",
    "# df2.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### **Teste de Named Entity Recognition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment 1:  Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "Sentiment 2:  Sentiment(polarity=-0.6999999999999998, subjectivity=0.6666666666666666)\n"
     ]
    }
   ],
   "source": [
    "phrase = \"#BTC looks like it's going to crash again, so I'm just going to wait and see what happens.\"\n",
    "sentiment = TextBlob(phrase).sentiment\n",
    "print(\"Sentiment 1: \", sentiment)\n",
    "\n",
    "phrase = \"#BTC looks like it's going to go bad again, so I'm just going to wait and see what happens.\"\n",
    "sentiment = TextBlob(phrase).sentiment\n",
    "print(\"Sentiment 2: \", sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment 1:  -0.0516\n",
      "Sentiment 2:  -0.25\n"
     ]
    }
   ],
   "source": [
    "phrase = \"#BTC looks like it's going to crash again, so I'm just going to wait and see what happens.\"\n",
    "sentiment = analyzer.polarity_scores(phrase)\n",
    "print(\"Sentiment 1: \", sentiment[\"compound\"])\n",
    "\n",
    "phrase = \"#BTC looks like it's going to go bad again, so I'm just going to wait and see what happens.\"\n",
    "sentiment = analyzer.polarity_scores(phrase)\n",
    "print(\"Sentiment 2: \", sentiment[\"compound\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### TODO:\n",
    "\n",
    "- Utilizar uma palavra (tipo \"money\") para substituir pelo BTC, #BTC, Bitcoin, etc.. para verificar se o Vader e o TextBlob conseguem extrair conhecimento com isso, já que é uma palavra que ele deve conhecer o significado e ver se melhora os resultados - Pesquisar sobre Named Entity Recognition\n",
    "\n",
    "- Instalar NLTK (nltk.corpus, nltk.tokenize, nltk.probability, word_tokenize) [Ver este link](https://www.analyticsvidhya.com/blog/2021/06/vader-for-sentiment-analysis/)\n",
    "\n",
    "- Utilizar api chatgpt para obter sentimento\n",
    "\n",
    "- Comparar sentimento com NLTK vs Sentimento sem NLTK"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DAA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
