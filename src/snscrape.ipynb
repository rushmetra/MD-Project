{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install snscrape\n",
    "!pip install textblob\n",
    "!pip install pandas\n",
    "!pip install vaderSentiment\n",
    "!pip install tqdm\n",
    "!pip install nltk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Needed imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from tqdm import tnrange, tqdm_notebook, tqdm\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Start Mining Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_49723/3342589359.py:13: DeprecatedFeatureWarning: content is deprecated, use rawContent instead\n",
      "  tweets.append([tweet.date, tweet.url, tweet.user.username, tweet.sourceLabel, tweet.user.location, tweet.content, tweet.likeCount, tweet.retweetCount,  tweet.quoteCount, tweet.replyCount])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (1000, 10)\n"
     ]
    }
   ],
   "source": [
    "query = \"(crash, OR crashing, OR cair, OR queda, OR subir, OR subida, OR bullish, OR bearish, OR explode, OR exploding) -#BTC -#SafeBlast -#bitcoin -#SOL -#solana -#ADA -#XRP -#SHIB -#BNB -giveaway -congrats -congratulations -giving -link (#eth) until:2022-09-15 since:2022-08-15\"\n",
    "# query = \"(crash, OR crashing, OR cair, OR queda, OR subir, OR subida, OR bullish, OR bearish, OR explode, OR exploding) -#BTC -#SafeBlast -#bitcoin -#SOL -#solana -#ADA -#XRP -#SHIB -#BNB -giveaway -giveaways -congrats -congratulations -winner -giving -link -https -telegram (#eth) until:2022-09-15 since:2022-08-15\"\n",
    "tweets = []\n",
    "limit = 1000\n",
    "\n",
    "#TODO: meter aqui a barra de progresso ( https://github.com/tqdm/tqdm )\n",
    "\n",
    "for tweet in sntwitter.TwitterHashtagScraper(query).get_items():\n",
    "    \n",
    "    if len(tweets) == limit:\n",
    "        break\n",
    "    else:\n",
    "        tweets.append([tweet.date, tweet.url, tweet.user.username, tweet.sourceLabel, tweet.user.location, tweet.content, tweet.likeCount, tweet.retweetCount,  tweet.quoteCount, tweet.replyCount])\n",
    "        \n",
    "df = pd.DataFrame(tweets, columns=['Date', 'TweetURL','User', 'Source', 'Location', 'Tweet', 'Likes_Count','Retweet_Count', 'Quote_Count', 'Reply_Count'])\n",
    "\n",
    "df.to_csv('../data/bullishTweets.csv')\n",
    "\n",
    "print(\"Shape: \", df.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis with VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 3366.57it/s]\n"
     ]
    }
   ],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "compound = []\n",
    "for i,s in enumerate(tqdm(df['Tweet'])):\n",
    "    vs = analyzer.polarity_scores(s)\n",
    "    compound.append(vs[\"compound\"])\n",
    "df[\"compoundVader\"] = compound\n",
    "df.head(2)\n",
    "\n",
    "df.to_csv('../data/compoundAnalysis.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis with TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1779.64it/s]\n"
     ]
    }
   ],
   "source": [
    "compound = []\n",
    "for i,s in enumerate(tqdm(df['Tweet'])):\n",
    "    vs = TextBlob(s).sentiment\n",
    "    compound.append(vs)\n",
    "df[\"compoundTextBlob\"] = compound\n",
    "df.head(2)\n",
    "\n",
    "df.to_csv('../data/compoundAnalysis.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sort vader compound values by descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.sort_values(by=['compoundVader'], ascending=False)\n",
    "df2.to_csv('../data/orderedAnalysis.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate mean compound value (pensar numa maneira melhor de ver isto, mas para já faz o serviço)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  0.15064550000000168\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for x in df2['compoundVader']:\n",
    "    i += x\n",
    "\n",
    "mean = i/len(df2['compoundVader'])\n",
    "print(\"Mean: \", mean)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Aplicar o NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### **Teste de Named Entity Recognition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment 1:  Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "Sentiment 2:  Sentiment(polarity=-0.6999999999999998, subjectivity=0.6666666666666666)\n"
     ]
    }
   ],
   "source": [
    "phrase = \"#BTC looks like it's going to crash again, so I'm just going to wait and see what happens.\"\n",
    "sentiment = TextBlob(phrase).sentiment\n",
    "print(\"Sentiment 1: \", sentiment)\n",
    "\n",
    "phrase = \"#BTC looks like it's going to go bad again, so I'm just going to wait and see what happens.\"\n",
    "sentiment = TextBlob(phrase).sentiment\n",
    "print(\"Sentiment 2: \", sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment 1:  -0.0516\n",
      "Sentiment 2:  -0.25\n"
     ]
    }
   ],
   "source": [
    "phrase = \"#BTC looks like it's going to crash again, so I'm just going to wait and see what happens.\"\n",
    "sentiment = analyzer.polarity_scores(phrase)\n",
    "print(\"Sentiment 1: \", sentiment[\"compound\"])\n",
    "\n",
    "phrase = \"#BTC looks like it's going to go bad again, so I'm just going to wait and see what happens.\"\n",
    "sentiment = analyzer.polarity_scores(phrase)\n",
    "print(\"Sentiment 2: \", sentiment[\"compound\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### TODO:\n",
    "\n",
    "- Ordenar por sentimento e verificar se corresponde\n",
    "\n",
    "- Utilizar uma palavra (tipo \"money\") para substituir pelo BTC, #BTC, Bitcoin, etc.. para verificar se o Vader e o TextBlob conseguem extrair conhecimento com isso, já que é uma palavra que ele deve conhecer o significado e ver se melhora os resultados - Pesquisar sobre Named Entity Recognition\n",
    "\n",
    "- Fazer os resultados manualmente para 5 ou 10 tweets (que sejam explicitos sobre o seu sentimento) e comparar com os valores previstos pelo Vader e o TextBlob para ver se as falhas nos resultados são deles ou dos Tweets que não dizem merda nenhuma de jeito. Aproveitar para justificar isso no relatório\n",
    "\n",
    "- Instalar NLTK (nltk.corpus, nltk.tokenize, nltk.probability, word_tokenize) [Ver este link](https://www.analyticsvidhya.com/blog/2021/06/vader-for-sentiment-analysis/)\n",
    "\n",
    "- Verficar tweets nulos, sem conteudo, etc...\n",
    "\n",
    "- Verificar a quantidade de interações\n",
    "\n",
    "- Meter o tqsm a funcionar no scapping dos tweets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DAA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
