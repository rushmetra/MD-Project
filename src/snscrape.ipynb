{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install snscrape\n",
    "# !pip install textblob\n",
    "# !pip install pandas\n",
    "# !pip install vaderSentiment\n",
    "# !pip install tqdm\n",
    "# !pip install nltk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Needed imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from tqdm import tnrange, tqdm_notebook, tqdm\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import regex as re\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Start Mining Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"(crash, OR crashing, OR cair, OR queda, OR subir, OR subida, OR bullish, OR bearish, OR explode, OR exploding) -#BTC -#SafeBlast -#bitcoin -#SOL -#solana -#ADA -#XRP -#SHIB -#BNB -giveaway -congrats -congratulations -giving -link (#eth) until:2022-09-15 since:2022-08-15\"\n",
    "# # query = \"(crash, OR crashing, OR cair, OR queda, OR subir, OR subida, OR bullish, OR bearish, OR explode, OR exploding) -#BTC -#SafeBlast -#bitcoin -#SOL -#solana -#ADA -#XRP -#SHIB -#BNB -giveaway -giveaways -congrats -congratulations -winner -giving -link -https -telegram (#eth) until:2022-09-15 since:2022-08-15\"\n",
    "# tweets = []\n",
    "# limit = 1000\n",
    "\n",
    "# for tweet in sntwitter.TwitterHashtagScraper(query).get_items():\n",
    "    \n",
    "#     if len(tweets) == limit:\n",
    "#         break\n",
    "#     else:\n",
    "#         tweets.append([tweet.date, tweet.url, tweet.user.username, tweet.sourceLabel, tweet.user.location, tweet.content, tweet.likeCount, tweet.retweetCount,  tweet.quoteCount, tweet.replyCount])\n",
    "        \n",
    "# df = pd.DataFrame(tweets, columns=['Date', 'TweetURL','User', 'Source', 'Location', 'Tweet', 'Likes_Count','Retweet_Count', 'Quote_Count', 'Reply_Count'])\n",
    "\n",
    "# df.to_csv('../data/bullishTweets.csv')\n",
    "\n",
    "# print(\"Shape: \", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/bullishTweets.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis with VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 8803.85it/s]\n"
     ]
    }
   ],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "compound = []\n",
    "for i,s in enumerate(tqdm(df['Tweet'])):\n",
    "    vs = analyzer.polarity_scores(s)\n",
    "    compound.append(vs[\"compound\"])\n",
    "df[\"compoundVader\"] = compound\n",
    "df.head(2)\n",
    "\n",
    "df.to_csv('../data/compoundAnalysis.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis with TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 3182.63it/s]\n"
     ]
    }
   ],
   "source": [
    "compound = []\n",
    "for i,s in enumerate(tqdm(df['Tweet'])):\n",
    "    vs = TextBlob(s).sentiment\n",
    "    compound.append(vs)\n",
    "df[\"compoundTextBlob\"] = compound\n",
    "df.head(2)\n",
    "\n",
    "df.to_csv('../data/compoundAnalysis.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sort vader compound values by descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.sort_values(by=['compoundVader'], ascending=False)\n",
    "df2.to_csv('../data/orderedAnalysis.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate mean compound value (pensar numa maneira melhor de ver isto, mas para já faz o serviço)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  0.15064550000000168\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for x in df2['compoundVader']:\n",
    "    i += x\n",
    "\n",
    "mean = i/len(df2['compoundVader'])\n",
    "print(\"Mean: \", mean)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Text Cleaning using regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleantxt(text):\n",
    "    text= re.sub(r'@[A-Za-z0-9]+', '',text)# removed @mentions\n",
    "    text= re.sub(r'#', '',text)# removed # symbol\n",
    "    text = re.sub(r'RT[\\s]+', '',text)# rmoved RT\n",
    "    text = re.sub(r'https?:\\/\\/\\s+', '',text)# removed the hyperlink\n",
    "    text = re.sub(r'\\w+:\\/\\/[a-zA-Z0-9.\\/-]+', '',text) # removed any other links (like telegram)\n",
    "    return text\n",
    "\n",
    "\n",
    "df2[\"Tweet\"] = df2[\"Tweet\"].apply(cleantxt)\n",
    "\n",
    "df2.to_csv('../data/cleanedTweets.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### NLTK pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import string\n",
    "\n",
    "# Definir as stopwords da língua portuguesa\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Definir o stemmer a ser utilizado\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "# Definir a função de pré-processamento\n",
    "def preprocessamento(tweet):\n",
    "    # Tokenização\n",
    "    tokens = word_tokenize(tweet.lower())\n",
    "    \n",
    "    # Remoção de stopwords e caracteres especiais\n",
    "    tokens = [token for token in tokens if (token not in stopwords) and (token not in string.punctuation)]\n",
    "    \n",
    "    # Stemming\n",
    "    tokens_stem = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    # Juntar tokens em uma string\n",
    "    tweet_preprocessado = \" \".join(tokens_stem)\n",
    "    \n",
    "    return tweet_preprocessado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar pré-processamento à coluna \"Tweets\"\n",
    "df2['Tweet_NLTK'] = df2['Tweet'].apply(preprocessamento)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Classifying as Positive or Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma nova coluna \"sentiment\" com valores \"positive\" ou \"negative\"\n",
    "df2[\"Sentiment\"] = [\"positive\" if x >= 0 else \"negative\" for x in df2[\"compoundVader\"]]\n",
    "\n",
    "\n",
    "df2.to_csv('../data/preprocessedTweets.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted classification for each Tweet considering num of interactions (likes and retweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = []\n",
    "# for i, s in tqdm(df2.iterrows(), total=df2.shape[0]):\n",
    "#     scores.append(s[\"compoundVader\"] * ((s[\"Likes_Count\"]+1))* ((s[\"Retweet_Count\"]+1)))\n",
    "# df2[\"score\"] = scores\n",
    "# df2.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### **Teste de Named Entity Recognition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment 1:  Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "Sentiment 2:  Sentiment(polarity=-0.6999999999999998, subjectivity=0.6666666666666666)\n"
     ]
    }
   ],
   "source": [
    "phrase = \"#BTC looks like it's going to crash again, so I'm just going to wait and see what happens.\"\n",
    "sentiment = TextBlob(phrase).sentiment\n",
    "print(\"Sentiment 1: \", sentiment)\n",
    "\n",
    "phrase = \"#BTC looks like it's going to go bad again, so I'm just going to wait and see what happens.\"\n",
    "sentiment = TextBlob(phrase).sentiment\n",
    "print(\"Sentiment 2: \", sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment 1:  -0.0516\n",
      "Sentiment 2:  -0.25\n"
     ]
    }
   ],
   "source": [
    "phrase = \"#BTC looks like it's going to crash again, so I'm just going to wait and see what happens.\"\n",
    "sentiment = analyzer.polarity_scores(phrase)\n",
    "print(\"Sentiment 1: \", sentiment[\"compound\"])\n",
    "\n",
    "phrase = \"#BTC looks like it's going to go bad again, so I'm just going to wait and see what happens.\"\n",
    "sentiment = analyzer.polarity_scores(phrase)\n",
    "print(\"Sentiment 2: \", sentiment[\"compound\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### TODO:\n",
    "\n",
    "- Ordenar por sentimento e verificar se corresponde\n",
    "\n",
    "- Utilizar uma palavra (tipo \"money\") para substituir pelo BTC, #BTC, Bitcoin, etc.. para verificar se o Vader e o TextBlob conseguem extrair conhecimento com isso, já que é uma palavra que ele deve conhecer o significado e ver se melhora os resultados - Pesquisar sobre Named Entity Recognition\n",
    "\n",
    "- Fazer os resultados manualmente para 5 ou 10 tweets (que sejam explicitos sobre o seu sentimento) e comparar com os valores previstos pelo Vader e o TextBlob para ver se as falhas nos resultados são deles ou dos Tweets que não dizem merda nenhuma de jeito. Aproveitar para justificar isso no relatório\n",
    "\n",
    "- Instalar NLTK (nltk.corpus, nltk.tokenize, nltk.probability, word_tokenize) [Ver este link](https://www.analyticsvidhya.com/blog/2021/06/vader-for-sentiment-analysis/)\n",
    "\n",
    "- Verficar tweets nulos, sem conteudo, etc...\n",
    "\n",
    "- Verificar a quantidade de interações\n",
    "\n",
    "- Meter o tqsm a funcionar no scapping dos tweets\n",
    "\n",
    "- Utilizar o PyMc para ver obter uma modelagem estatistica no final (falar com o professor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DAA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
