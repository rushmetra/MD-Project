{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install snscrape\n",
    "!pip install textblob\n",
    "!pip install pandas\n",
    "!pip install vaderSentiment\n",
    "!pip install tqdm\n",
    "!pip install nltk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Needed imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from tqdm import tnrange, tqdm_notebook, tqdm\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import regex as re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Start Mining Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error retrieving https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&include_ext_has_nft_avatar=1&include_ext_is_blue_verified=1&include_ext_verified_type=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_ext_limited_action_results=false&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_ext_collab_control=true&include_ext_views=true&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&include_ext_sensitive_media_warning=true&include_ext_trusted_friends_metadata=true&send_error_codes=true&simple_quoted_tweet=true&q=%23%28crash%2C+OR+crashing%2C+OR+cair%2C+OR+queda%2C+OR+subir%2C+OR+subida%2C+OR+bullish%2C+OR+bearish%2C+OR+explode%2C+OR+exploding%29+-%23BTC+-%23SafeBlast+-%23bitcoin+-%23SOL+-%23solana+-%23ADA+-%23XRP+-%23SHIB+-%23BNB+-giveaway+-congrats+-congratulations+-giving+-link+%28%23eth%29+until%3A2022-09-15+since%3A2022-08-15&tweet_search_mode=live&count=20&query_source=spelling_expansion_revert_click&pc=1&spelling_corrections=1&include_ext_edit_control=true&ext=mediaStats%2ChighlightedLabel%2ChasNftAvatar%2CvoiceInfo%2Cenrichments%2CsuperFollowMetadata%2CunmentionInfo%2CeditControl%2Ccollab_control%2Cvibe: blocked (403)\n",
      "4 requests to https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&include_ext_has_nft_avatar=1&include_ext_is_blue_verified=1&include_ext_verified_type=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_ext_limited_action_results=false&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_ext_collab_control=true&include_ext_views=true&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&include_ext_sensitive_media_warning=true&include_ext_trusted_friends_metadata=true&send_error_codes=true&simple_quoted_tweet=true&q=%23%28crash%2C+OR+crashing%2C+OR+cair%2C+OR+queda%2C+OR+subir%2C+OR+subida%2C+OR+bullish%2C+OR+bearish%2C+OR+explode%2C+OR+exploding%29+-%23BTC+-%23SafeBlast+-%23bitcoin+-%23SOL+-%23solana+-%23ADA+-%23XRP+-%23SHIB+-%23BNB+-giveaway+-congrats+-congratulations+-giving+-link+%28%23eth%29+until%3A2022-09-15+since%3A2022-08-15&tweet_search_mode=live&count=20&query_source=spelling_expansion_revert_click&pc=1&spelling_corrections=1&include_ext_edit_control=true&ext=mediaStats%2ChighlightedLabel%2ChasNftAvatar%2CvoiceInfo%2Cenrichments%2CsuperFollowMetadata%2CunmentionInfo%2CeditControl%2Ccollab_control%2Cvibe failed, giving up.\n",
      "Errors: blocked (403), blocked (403), blocked (403), blocked (403)\n"
     ]
    },
    {
     "ename": "ScraperException",
     "evalue": "4 requests to https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&include_ext_has_nft_avatar=1&include_ext_is_blue_verified=1&include_ext_verified_type=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_ext_limited_action_results=false&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_ext_collab_control=true&include_ext_views=true&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&include_ext_sensitive_media_warning=true&include_ext_trusted_friends_metadata=true&send_error_codes=true&simple_quoted_tweet=true&q=%23%28crash%2C+OR+crashing%2C+OR+cair%2C+OR+queda%2C+OR+subir%2C+OR+subida%2C+OR+bullish%2C+OR+bearish%2C+OR+explode%2C+OR+exploding%29+-%23BTC+-%23SafeBlast+-%23bitcoin+-%23SOL+-%23solana+-%23ADA+-%23XRP+-%23SHIB+-%23BNB+-giveaway+-congrats+-congratulations+-giving+-link+%28%23eth%29+until%3A2022-09-15+since%3A2022-08-15&tweet_search_mode=live&count=20&query_source=spelling_expansion_revert_click&pc=1&spelling_corrections=1&include_ext_edit_control=true&ext=mediaStats%2ChighlightedLabel%2ChasNftAvatar%2CvoiceInfo%2Cenrichments%2CsuperFollowMetadata%2CunmentionInfo%2CeditControl%2Ccollab_control%2Cvibe failed, giving up.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mScraperException\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/josejoao/Universidade/4ano/2semestre/MD/MD-Project/src/snscrape.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/josejoao/Universidade/4ano/2semestre/MD/MD-Project/src/snscrape.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m limit \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/josejoao/Universidade/4ano/2semestre/MD/MD-Project/src/snscrape.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m#TODO: meter aqui a barra de progresso ( https://github.com/tqdm/tqdm )\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/josejoao/Universidade/4ano/2semestre/MD/MD-Project/src/snscrape.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m tweet \u001b[39min\u001b[39;00m sntwitter\u001b[39m.\u001b[39mTwitterHashtagScraper(query)\u001b[39m.\u001b[39mget_items():\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/josejoao/Universidade/4ano/2semestre/MD/MD-Project/src/snscrape.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(tweets) \u001b[39m==\u001b[39m limit:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/josejoao/Universidade/4ano/2semestre/MD/MD-Project/src/snscrape.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/snscrape/modules/twitter.py:1661\u001b[0m, in \u001b[0;36mTwitterSearchScraper.get_items\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1658\u001b[0m params \u001b[39m=\u001b[39m paginationParams\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m   1659\u001b[0m \u001b[39mdel\u001b[39;00m params[\u001b[39m'\u001b[39m\u001b[39mcursor\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m-> 1661\u001b[0m \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter_api_data(\u001b[39m'\u001b[39m\u001b[39mhttps://api.twitter.com/2/search/adaptive.json\u001b[39m\u001b[39m'\u001b[39m, _TwitterAPIType\u001b[39m.\u001b[39mV2, params, paginationParams, cursor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cursor):\n\u001b[1;32m   1662\u001b[0m \t\u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_v2_timeline_instructions_to_tweets_or_users(obj)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/snscrape/modules/twitter.py:761\u001b[0m, in \u001b[0;36m_TwitterAPIScraper._iter_api_data\u001b[0;34m(self, endpoint, apiType, params, paginationParams, cursor, direction)\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    760\u001b[0m \t_logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mRetrieving scroll page \u001b[39m\u001b[39m{\u001b[39;00mcursor\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 761\u001b[0m \tobj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_api_data(endpoint, apiType, reqParams)\n\u001b[1;32m    762\u001b[0m \t\u001b[39myield\u001b[39;00m obj\n\u001b[1;32m    764\u001b[0m \t\u001b[39m# No data format test, just a hard and loud crash if anything's wrong :-)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/snscrape/modules/twitter.py:727\u001b[0m, in \u001b[0;36m_TwitterAPIScraper._get_api_data\u001b[0;34m(self, endpoint, apiType, params)\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[39mif\u001b[39;00m apiType \u001b[39mis\u001b[39;00m _TwitterAPIType\u001b[39m.\u001b[39mGRAPHQL:\n\u001b[1;32m    726\u001b[0m \tparams \u001b[39m=\u001b[39m urllib\u001b[39m.\u001b[39mparse\u001b[39m.\u001b[39murlencode({k: json\u001b[39m.\u001b[39mdumps(v, separators \u001b[39m=\u001b[39m (\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m:\u001b[39m\u001b[39m'\u001b[39m)) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m params\u001b[39m.\u001b[39mitems()}, quote_via \u001b[39m=\u001b[39m urllib\u001b[39m.\u001b[39mparse\u001b[39m.\u001b[39mquote)\n\u001b[0;32m--> 727\u001b[0m r \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get(endpoint, params \u001b[39m=\u001b[39;49m params, headers \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apiHeaders, responseOkCallback \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_api_response)\n\u001b[1;32m    728\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    729\u001b[0m \tobj \u001b[39m=\u001b[39m r\u001b[39m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/snscrape/base.py:251\u001b[0m, in \u001b[0;36mScraper._get\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 251\u001b[0m \t\u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\u001b[39m'\u001b[39;49m\u001b[39mGET\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/snscrape/base.py:247\u001b[0m, in \u001b[0;36mScraper._request\u001b[0;34m(self, method, url, params, data, headers, timeout, responseOkCallback, allowRedirects, proxies)\u001b[0m\n\u001b[1;32m    245\u001b[0m \t_logger\u001b[39m.\u001b[39mfatal(msg)\n\u001b[1;32m    246\u001b[0m \t_logger\u001b[39m.\u001b[39mfatal(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mErrors: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(errors)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 247\u001b[0m \t\u001b[39mraise\u001b[39;00m ScraperException(msg)\n\u001b[1;32m    248\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mReached unreachable code\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mScraperException\u001b[0m: 4 requests to https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&include_ext_has_nft_avatar=1&include_ext_is_blue_verified=1&include_ext_verified_type=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_ext_limited_action_results=false&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_ext_collab_control=true&include_ext_views=true&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&include_ext_sensitive_media_warning=true&include_ext_trusted_friends_metadata=true&send_error_codes=true&simple_quoted_tweet=true&q=%23%28crash%2C+OR+crashing%2C+OR+cair%2C+OR+queda%2C+OR+subir%2C+OR+subida%2C+OR+bullish%2C+OR+bearish%2C+OR+explode%2C+OR+exploding%29+-%23BTC+-%23SafeBlast+-%23bitcoin+-%23SOL+-%23solana+-%23ADA+-%23XRP+-%23SHIB+-%23BNB+-giveaway+-congrats+-congratulations+-giving+-link+%28%23eth%29+until%3A2022-09-15+since%3A2022-08-15&tweet_search_mode=live&count=20&query_source=spelling_expansion_revert_click&pc=1&spelling_corrections=1&include_ext_edit_control=true&ext=mediaStats%2ChighlightedLabel%2ChasNftAvatar%2CvoiceInfo%2Cenrichments%2CsuperFollowMetadata%2CunmentionInfo%2CeditControl%2Ccollab_control%2Cvibe failed, giving up."
     ]
    }
   ],
   "source": [
    "query = \"(crash, OR crashing, OR cair, OR queda, OR subir, OR subida, OR bullish, OR bearish, OR explode, OR exploding) -#BTC -#SafeBlast -#bitcoin -#SOL -#solana -#ADA -#XRP -#SHIB -#BNB -giveaway -congrats -congratulations -giving -link (#eth) until:2022-09-15 since:2022-08-15\"\n",
    "# query = \"(crash, OR crashing, OR cair, OR queda, OR subir, OR subida, OR bullish, OR bearish, OR explode, OR exploding) -#BTC -#SafeBlast -#bitcoin -#SOL -#solana -#ADA -#XRP -#SHIB -#BNB -giveaway -giveaways -congrats -congratulations -winner -giving -link -https -telegram (#eth) until:2022-09-15 since:2022-08-15\"\n",
    "tweets = []\n",
    "limit = 1000\n",
    "\n",
    "#TODO: meter aqui a barra de progresso ( https://github.com/tqdm/tqdm )\n",
    "\n",
    "for tweet in sntwitter.TwitterHashtagScraper(query).get_items():\n",
    "    \n",
    "    if len(tweets) == limit:\n",
    "        break\n",
    "    else:\n",
    "        tweets.append([tweet.date, tweet.url, tweet.user.username, tweet.sourceLabel, tweet.user.location, tweet.content, tweet.likeCount, tweet.retweetCount,  tweet.quoteCount, tweet.replyCount])\n",
    "        \n",
    "df = pd.DataFrame(tweets, columns=['Date', 'TweetURL','User', 'Source', 'Location', 'Tweet', 'Likes_Count','Retweet_Count', 'Quote_Count', 'Reply_Count'])\n",
    "\n",
    "df.to_csv('../data/bullishTweets.csv')\n",
    "\n",
    "print(\"Shape: \", df.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis with VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 3366.57it/s]\n"
     ]
    }
   ],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "compound = []\n",
    "for i,s in enumerate(tqdm(df['Tweet'])):\n",
    "    vs = analyzer.polarity_scores(s)\n",
    "    compound.append(vs[\"compound\"])\n",
    "df[\"compoundVader\"] = compound\n",
    "df.head(2)\n",
    "\n",
    "df.to_csv('../data/compoundAnalysis.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis with TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1779.64it/s]\n"
     ]
    }
   ],
   "source": [
    "compound = []\n",
    "for i,s in enumerate(tqdm(df['Tweet'])):\n",
    "    vs = TextBlob(s).sentiment\n",
    "    compound.append(vs)\n",
    "df[\"compoundTextBlob\"] = compound\n",
    "df.head(2)\n",
    "\n",
    "df.to_csv('../data/compoundAnalysis.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sort vader compound values by descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.sort_values(by=['compoundVader'], ascending=False)\n",
    "df2.to_csv('../data/orderedAnalysis.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate mean compound value (pensar numa maneira melhor de ver isto, mas para já faz o serviço)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  0.15064550000000168\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for x in df2['compoundVader']:\n",
    "    i += x\n",
    "\n",
    "mean = i/len(df2['compoundVader'])\n",
    "print(\"Mean: \", mean)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Aplicar pré processamento & NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/orderedAnalysis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply text cleaning using regex expressions\n",
    "def cleantxt(text):\n",
    "    text= re.sub(r'@[A-Za-z0-9]+', '',text)# removed @mentions\n",
    "    # text= re.sub(r'#', '',text)# removed # symbol\n",
    "    text = re.sub(r'RT[\\s]+', '',text)# rmoved RT\n",
    "    text = re.sub(r'https?:\\/\\/\\s+', '',text)# removed the hyperlink\n",
    "    # text = re.sub(r':+', '',text)# removed : symbol\n",
    "    # text = re.sub(r'--+', '',text)# removed : symbol\n",
    "    text = re.sub(r'\\w+:\\/\\/[a-zA-Z0-9.\\/-]+', '',text)\n",
    "    return text\n",
    "\n",
    "\n",
    "data[\"Tweet\"] = data[\"Tweet\"].apply(cleantxt)\n",
    "\n",
    "data.to_csv('../data/cleanedTweets.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### **Teste de Named Entity Recognition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment 1:  Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "Sentiment 2:  Sentiment(polarity=-0.6999999999999998, subjectivity=0.6666666666666666)\n"
     ]
    }
   ],
   "source": [
    "phrase = \"#BTC looks like it's going to crash again, so I'm just going to wait and see what happens.\"\n",
    "sentiment = TextBlob(phrase).sentiment\n",
    "print(\"Sentiment 1: \", sentiment)\n",
    "\n",
    "phrase = \"#BTC looks like it's going to go bad again, so I'm just going to wait and see what happens.\"\n",
    "sentiment = TextBlob(phrase).sentiment\n",
    "print(\"Sentiment 2: \", sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment 1:  -0.0516\n",
      "Sentiment 2:  -0.25\n"
     ]
    }
   ],
   "source": [
    "phrase = \"#BTC looks like it's going to crash again, so I'm just going to wait and see what happens.\"\n",
    "sentiment = analyzer.polarity_scores(phrase)\n",
    "print(\"Sentiment 1: \", sentiment[\"compound\"])\n",
    "\n",
    "phrase = \"#BTC looks like it's going to go bad again, so I'm just going to wait and see what happens.\"\n",
    "sentiment = analyzer.polarity_scores(phrase)\n",
    "print(\"Sentiment 2: \", sentiment[\"compound\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### TODO:\n",
    "\n",
    "- Ordenar por sentimento e verificar se corresponde\n",
    "\n",
    "- Utilizar uma palavra (tipo \"money\") para substituir pelo BTC, #BTC, Bitcoin, etc.. para verificar se o Vader e o TextBlob conseguem extrair conhecimento com isso, já que é uma palavra que ele deve conhecer o significado e ver se melhora os resultados - Pesquisar sobre Named Entity Recognition\n",
    "\n",
    "- Fazer os resultados manualmente para 5 ou 10 tweets (que sejam explicitos sobre o seu sentimento) e comparar com os valores previstos pelo Vader e o TextBlob para ver se as falhas nos resultados são deles ou dos Tweets que não dizem merda nenhuma de jeito. Aproveitar para justificar isso no relatório\n",
    "\n",
    "- Instalar NLTK (nltk.corpus, nltk.tokenize, nltk.probability, word_tokenize) [Ver este link](https://www.analyticsvidhya.com/blog/2021/06/vader-for-sentiment-analysis/)\n",
    "\n",
    "- Verficar tweets nulos, sem conteudo, etc...\n",
    "\n",
    "- Verificar a quantidade de interações\n",
    "\n",
    "- Meter o tqsm a funcionar no scapping dos tweets\n",
    "\n",
    "- Utilizar o PyMc para ver obter uma modelagem estatistica no final (falar com o professor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DAA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
